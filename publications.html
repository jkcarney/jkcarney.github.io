<!DOCTYPE html>
<html lang="en">

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Joshua Carney's Portfolio Website">
    <meta name="keywords"
        content="Joshua Carney, Millersville University, Carney, Unity Developer, Python, AI Developer">
    <title>Joshua Carney's Publications</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    <link rel="stylesheet" href="style.css">

    <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>

    <script src="https://unpkg.com/popper.js@1/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@4"></script>
</head>

<body class="front font-sans antialiased text-gray-100 leading-normal tracking-wider bg-cover"
    style="background-image:url('svgs/starry_night_rhone_2600circles.svg');">
    <section class="text-gray-100 body-font">
        <div class="container px-5 py-24 pb-24 mx-auto">
            <div
                class="flex flex-col text-center mb-12 rounded-3xl bg-gradient-to-r from-gray-700 via-gray-800 to-gray-700 bg-opacity-80 shadow-lg p-6 border border-gray-600">
                <div class="flex justify-center mb-2">
                    <div class="h-1 w-16 bg-blue-500 rounded-full"></div>
                </div>
                <h1 class="sm:text-4xl text-3xl font-bold title-font text-gray-100 tracking-wide">
                    Publications
                </h1>
                <p class="text-sm text-gray-400 mt-2">they don't call me a <i>research</i> software engineer for nothing :)</p>
            </div>

            <div class="flex flex-wrap -m-4">
                <!-- Example publication item -->
                <div class="w-full md:w-3/4 lg:w-3/4 p-4 mx-auto">
                    <div
                        class="h-full bg-gray-800 bg-opacity-90 border-2 border-gray-200 rounded-3xl shadow-xl p-4 flex flex-col md:flex-row">

                        <!-- TEXT -->
                        <div class="md:flex-1 md:pr-4">
                            <a href="https://arxiv.org/abs/2506.06518" target="_blank">
                                <h2 class="text-xl font-bold text-gray-100 underline hover:text-blue-500 mb-2">
                                    A Systematic Review of Poisoning Attacks Against Large Language Models
                                </h2>
                            </a>
                            <p class="text-indigo-200 text-sm mb-1">
                                <strong>Authors:</strong> Neil Fendley, Edward W. Staley, <i>Joshua Carney</i>, William
                                Redman, Marie Chau, Nathan Drenkow
                            </p>

                            <p class="text-indigo-200 text-sm mb-4">
                                <strong>Year:</strong> 2025
                            </p>
                            <p class="text-gray-300 text-sm leading-relaxed">
                                With the widespread availability of pretrained Large Language Models (LLMs) and their
                                training datasets, concerns about the security risks associated with their usage has
                                increased significantly. One of these security risks is the threat of LLM poisoning
                                attacks where an attacker modifies some part of the LLM training process to cause the
                                LLM to behave in a malicious way. As an emerging area of research, the current
                                frameworks and terminology for LLM poisoning attacks are derived from earlier
                                classification poisoning literature and are not fully equipped for generative LLM
                                settings. We conduct a systematic review of published LLM poisoning attacks to clarify
                                the security implications and address inconsistencies in terminology across the
                                literature. We propose a comprehensive poisoning threat model applicable to categorize a
                                wide range of LLM poisoning attacks. The poisoning threat model includes four poisoning
                                attack specifications that define the logistics and manipulation strategies of an attack
                                as well as six poisoning metrics used to measure key characteristics of an attack. Under
                                our proposed framework, we organize our discussion of published LLM poisoning literature
                                along four critical dimensions of LLM poisoning attacks: concept poisons, stealthy
                                poisons, persistent poisons, and poisons for unique tasks, to better understand the
                                current landscape of security risks.
                            </p>
                            <div class="mt-4">
                                <a href="https://arxiv.org/abs/2506.06518" target="_blank"
                                    class="text-blue-400 text-sm underline hover:text-blue-500">
                                    arXiv
                                </a>
                            </div>
                        </div>

                        <!-- IMAGE -->
                        <div class="md:w-60 md:flex-shrink-0 mt-4 md:mt-0 flex justify-center items-start">
                            <img src="images/pubs/systematic_review.png" alt="Publication diagram"
                                class="rounded-lg border border-gray-500 object-cover aspect-square w-full">
                        </div>

                    </div>
                </div>

                <div class="w-full md:w-3/4 lg:w-3/4 p-4 mx-auto">
                    <div
                        class="h-full bg-gray-800 bg-opacity-90 border-2 border-gray-200 rounded-3xl shadow-xl p-4 flex flex-col md:flex-row">

                        <!-- TEXT -->
                        <div class="md:flex-1 md:pr-4">
                            <a href="https://arxiv.org/abs/2505.00734" target="_blank">
                                <h2 class="text-xl font-bold text-gray-100 underline hover:text-blue-500 mb-2">
                                    Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes
                                </h2>
                            </a>
                            <p class="text-indigo-200 text-sm mb-1">
                                <strong>Authors:</strong> Neil Joshi, <i>Joshua Carney</i>, Nathanael Kuo, Homer Li,
                                Cheng Peng, Myron Brown
                            </p>
                            <p class="text-indigo-200 text-sm mb-4">
                                <strong>Year:</strong> 2025
                            </p>
                            <p class="text-gray-300 text-sm leading-relaxed">
                                Production of photorealistic, navigable 3D site models requires a large volume of
                                carefully collected images that are often unavailable to first responders for disaster
                                relief or law enforcement. Real-world challenges include limited numbers of images,
                                heterogeneous unposed cameras, inconsistent lighting, and extreme viewpoint differences
                                for images collected from varying altitudes. To promote research aimed at addressing
                                these challenges, we have developed the first public benchmark dataset for 3D
                                reconstruction and novel view synthesis based on multiple calibrated ground-level,
                                security-level, and airborne cameras. We present datasets that pose real-world
                                challenges, independently evaluate calibration of unposed cameras and quality of novel
                                rendered views, demonstrate baseline performance using recent state-of-practice methods,
                                and identify challenges for further research.
                            </p>
                            <div class="mt-4">
                                <a href="https://arxiv.org/abs/2505.00734" target="_blank"
                                    class="text-blue-400 text-sm underline hover:text-blue-500">
                                    arXiv
                                </a>
                            </div>
                        </div>

                        <!-- IMAGE -->
                        <div class="md:w-60 md:flex-shrink-0 mt-4 md:mt-0 flex justify-center items-start">
                            <img src="images/pubs/uls3d.png" alt="Publication diagram"
                                class="rounded-lg border border-gray-500 object-cover aspect-square w-full">
                        </div>

                    </div>
                </div>

                <div class="w-full md:w-3/4 lg:w-3/4 p-4 mx-auto">
                    <div
                        class="h-full bg-gray-800 bg-opacity-90 border-2 border-gray-200 rounded-3xl shadow-xl p-4 flex flex-col md:flex-row">

                        <!-- TEXT -->
                        <div class="md:flex-1 md:pr-4">
                            <a href="https://arxiv.org/abs/2505.17248" target="_blank">
                                <h2 class="text-xl font-bold text-gray-100 underline hover:text-blue-500 mb-2">
                                    Backdoors in DRL: Four Environments Focusing on In-distribution Triggers
                                </h2>
                            </a>
                            <p class="text-indigo-200 text-sm mb-1">
                                <strong>Authors:</strong> Chace Ashcraft, Ted Staley, <i>Josh Carney</i>, Cameron
                                Hickert, Kiran Karra, Nathan Drenkow
                            </p>
                            <p class="text-indigo-200 text-sm mb-4">
                                <strong>Year:</strong> 2025
                            </p>
                            <p class="text-gray-300 text-sm leading-relaxed">
                                Backdoor attacks, or trojans, pose a security risk by concealing undesirable behavior in
                                deep neural network models. Open-source neural networks are downloaded from the internet
                                daily, possibly containing backdoors, and third-party model developers are common. To
                                advance research on backdoor attack mitigation, we develop several trojans for deep
                                reinforcement learning (DRL) agents. We focus on in-distribution triggers, which occur
                                within the agent's natural data distribution, since they pose a more significant
                                security threat than out-of-distribution triggers due to their ease of activation by the
                                attacker during model deployment. We implement backdoor attacks in four reinforcement
                                learning (RL) environments: LavaWorld, Randomized LavaWorld, Colorful Memory, and
                                Modified Safety Gymnasium. We train various models, both clean and backdoored, to
                                characterize these attacks. We find that in-distribution triggers can require additional
                                effort to implement and be more challenging for models to learn, but are nevertheless
                                viable threats in DRL even using basic data poisoning attacks.
                            </p>
                            <div class="mt-4">
                                <a href="https://arxiv.org/abs/2505.17248" target="_blank"
                                    class="text-blue-400 text-sm underline hover:text-blue-500">
                                    arXiv
                                </a>
                            </div>
                        </div>

                        <!-- IMAGE -->
                        <div class="md:w-60 md:flex-shrink-0 mt-4 md:mt-0 flex justify-center items-start">
                            <img src="images/pubs/rl-triggers.png" alt="Publication diagram"
                                class="rounded-lg border border-gray-500 object-cover aspect-square w-full">
                        </div>

                    </div>
                </div>


                <div class="w-full md:w-3/4 lg:w-3/4 p-4 mx-auto">
                    <div
                        class="h-full bg-gray-800 bg-opacity-90 border-2 border-gray-200 rounded-3xl shadow-xl p-4 flex flex-col md:flex-row">

                        <!-- TEXT -->
                        <div class="md:flex-1 md:pr-4">
                            <a href="https://arxiv.org/abs/2504.08943" target="_blank">
                                <h2 class="text-xl font-bold text-gray-100 underline hover:text-blue-500 mb-2">
                                    Investigating the Treacherous Turn in Deep Reinforcement Learning
                                </h2>
                            </a>
                            <p class="text-indigo-200 text-sm mb-1">
                                <strong>Authors:</strong> Chace Ashcraft, Kiran Karra, <i>Josh Carney</i>, Nathan
                                Drenkow
                            </p>
                            <p class="text-indigo-200 text-sm mb-4">
                                <strong>Year:</strong> 2025
                            </p>
                            <p class="text-gray-300 text-sm leading-relaxed">
                                The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior. 
                            </p>
                            <div class="mt-4">
                                <a href="https://arxiv.org/abs/2504.08943" target="_blank"
                                    class="text-blue-400 text-sm underline hover:text-blue-500">
                                    arXiv
                                </a>
                            </div>
                        </div>

                        <!-- IMAGE -->
                        <div class="md:w-60 md:flex-shrink-0 mt-4 md:mt-0 flex justify-center items-start">
                            <img src="images/pubs/tt.png" alt="Publication diagram"
                                class="rounded-lg border border-gray-500 object-cover aspect-square w-full">
                        </div>

                    </div>
                </div>


    </section>
    <div
        class="flex flex-col text-center rounded-3xl bg-gradient-to-r from-gray-700 via-gray-800 to-gray-700 bg-opacity-80 shadow-lg border border-gray-600 p-4">
        <p class="flex items-center justify-center gap-2 text-base md:text-2xl text-blue-500 font-bold py-6">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" class="size-6">
                <path
                    d="M11.47 3.841a.75.75 0 0 1 1.06 0l8.69 8.69a.75.75 0 1 0 1.06-1.061l-8.689-8.69a2.25 2.25 0 0 0-3.182 0l-8.69 8.69a.75.75 0 1 0 1.061 1.06l8.69-8.689Z" />
                <path
                    d="m12 5.432 8.159 8.159c.03.03.06.058.091.086v6.198c0 1.035-.84 1.875-1.875 1.875H15a.75.75 0 0 1-.75-.75v-4.5a.75.75 0 0 0-.75-.75h-3a.75.75 0 0 0-.75.75V21a.75.75 0 0 1-.75.75H5.625a1.875 1.875 0 0 1-1.875-1.875v-6.198a2.29 2.29 0 0 0 .091-.086L12 5.432Z" />
            </svg>

            <a href="index.html" class="no-underline hover:underline">homepage</a>
        </p>
    </div>

</body>



</html>